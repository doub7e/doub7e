---
---

@article{li2025learning,
  title={Learning to Weight Parameters for Data Attribution},
  author={Li, Shuangqi and Le, Hieu and Xu, Jingyi and Salzmann, Mathieu},
  tldr={Attribution signals are noisy. Our method learns to re-weight layers to amplify the true signal, boosting accuracy and enabling fine-grained (e.g., subject vs. style) attribution.},
  abstract={We study data attribution in generative models, aiming to identify which training examples most influence a given output. Existing methods achieve this by tracing gradients back to training data. However, they typically treat all network parameters uniformly, ignoring the fact that different layers encode different types of information and may thus draw information differently from the training set. We propose a method that models this by learning parameter importance weights tailored for attribution, without requiring labeled data. This allows the attribution process to adapt to the structure of the model, capturing which training examples contribute to specific semantic aspects of an output, such as subject, style, or background. Our method improves attribution accuracy across diffusion models and enables fine-grained insights into how outputs borrow from training data.},
  journal={arXiv preprint arXiv:2506.05647},
  pdf={https://arxiv.org/pdf/2506.05647},
  year={2025},
  selected={true},
  preview="learning-to-weight-2.jpg",
}

@inproceedings{li2024enhancing,
  title={Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds},
  author={Li, Shuangqi and Le, Hieu and Xu, Jingyi and Salzmann, Mathieu},
  tldr={Noises are important for diffusion-based models. We show that some random seeds are much more reliable than others, which can be used to generate useful training data.},
  abstract={Text-to-image diffusion models have demonstrated remarkable capability in generating realistic images from arbitrary text prompts. However, they often produce inconsistent results for compositional prompts such as "two dogs" or "a penguin on the right of a bowl". Understanding these inconsistencies is crucial for reliable image generation. In this paper, we highlight the significant role of initial noise in these inconsistencies, where certain noise patterns are more reliable for compositional prompts than others. Our analyses reveal that different initial random seeds tend to guide the model to place objects in distinct image areas, potentially adhering to specific patterns of camera angles and image composition associated with the seed. To improve the model's compositional ability, we propose a method for mining these reliable cases, resulting in a curated training set of generated images without requiring any manual annotation. By fine-tuning text-to-image models on these generated images, we significantly enhance their compositional capabilities. For numerical composition, we observe relative increases of 29.3\% and 19.5\% for Stable Diffusion and PixArt-\alpha, respectively. Spatial composition sees even larger gains, with 60.7\% for Stable Diffusion and 21.1\% for PixArt-\alpha.},
  booktitle={The 13th International Conference on Learning Representations},
  presentation={Spotlight (top 4\%)},
  pdf={https://openreview.net/pdf?id=5BSlakturs},
  year={2024},
  selected={true},
  abbr={ICLR},
  code={https://github.com/doub7e/Reliable-Random-Seeds},
  preview="reliable-seeds.jpg",
}

@article{li2024controlling,
  title={Controlling the Fidelity and Diversity of Deep Generative Models via Pseudo Density},
  author={Li, Shuangqi and Liu, Chen and Zhang, Tong and Le, Hieu and S{\"u}sstrunk, Sabine and Salzmann, Mathieu},
  abstract={We introduce an approach to bias deep generative models, such as GANs and diffusion models, towards generating data with either enhanced fidelity or increased diversity. Our approach involves manipulating the distribution of training and generated data through a novel metric for individual samples, named pseudo density, which is based on the nearest-neighbor information from real samples. Our approach offers three distinct techniques to adjust the fidelity and diversity of deep generative models: 1) Per-sample perturbation, enabling precise adjustments for individual samples towards either more common or more unique characteristics; 2) Importance sampling during model inference to enhance either fidelity or diversity in the generated data; 3) Fine-tuning with importance sampling, which guides the generative model to learn an adjusted distribution, thus controlling fidelity and diversity. Furthermore, our fine-tuning method demonstrates the ability to improve the Frechet Inception Distance (FID) for pre-trained generative models with minimal iterations},
  journal={Transactions on Machine Learning Research},
  presentation={Selected for poster presentation at ICLR 2025},
  pdf={https://arxiv.org/pdf/2407.08659},
  year={2024},
  selected={true},
  abbr={TMLR},
  code={https://github.com/doub7e/Pseudo-Diversity},
  preview="pseudo-density.jpg",
}

@article{li2022interlock,
  title={Interlock-free multi-aspect rationalization for text classification},
  author={Li, Shuangqi and Antognini, Diego and Faltings, Boi},
  abstract={Explanation is important for text classification tasks. One prevalent type of explanation is rationales, which are text snippets of input text that suffice to yield the prediction and are meaningful to humans. A lot of research on rationalization has been based on the selective rationalization framework, which has recently been shown to be problematic due to the interlocking dynamics. In this paper, we show that we address the interlocking problem in the multi-aspect setting, where we aim to generate multiple rationales for multiple outputs. More specifically, we propose a multi-stage training method incorporating an additional self-supervised contrastive loss that helps to generate more semantically diverse rationales. Empirical results on the beer review dataset show that our method improves significantly the rationalization performance.},
  journal={arXiv preprint arXiv:2205.06756},
  pdf={https://arxiv.org/pdf/2205.06756},
  year={2022},
  preview="interlock-free.jpg",
  selected={true},
}


